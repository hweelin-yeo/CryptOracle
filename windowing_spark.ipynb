{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o53.load.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=127.0.0.1:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:179)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getServerAddressList(MongoClientDelegate.java:116)\n\tat com.mongodb.Mongo.getServerAddressList(Mongo.java:401)\n\tat com.mongodb.spark.connection.MongoClientCache.$anonfun$logClient$1(MongoClientCache.scala:161)\n\tat com.mongodb.spark.LoggingTrait.logInfo(LoggingTrait.scala:48)\n\tat com.mongodb.spark.LoggingTrait.logInfo$(LoggingTrait.scala:47)\n\tat com.mongodb.spark.Logging.logInfo(Logging.scala:24)\n\tat com.mongodb.spark.connection.MongoClientCache.logClient(MongoClientCache.scala:161)\n\tat com.mongodb.spark.connection.MongoClientCache.acquire(MongoClientCache.scala:56)\n\tat com.mongodb.spark.MongoConnector.acquireClient(MongoConnector.scala:239)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:152)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator$lzycompute(MongoRDD.scala:217)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator(MongoRDD.scala:217)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:68)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:240)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:229)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:179)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66d10ca3c353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmessages_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o53.load.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=127.0.0.1:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:179)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getServerAddressList(MongoClientDelegate.java:116)\n\tat com.mongodb.Mongo.getServerAddressList(Mongo.java:401)\n\tat com.mongodb.spark.connection.MongoClientCache.$anonfun$logClient$1(MongoClientCache.scala:161)\n\tat com.mongodb.spark.LoggingTrait.logInfo(LoggingTrait.scala:48)\n\tat com.mongodb.spark.LoggingTrait.logInfo$(LoggingTrait.scala:47)\n\tat com.mongodb.spark.Logging.logInfo(Logging.scala:24)\n\tat com.mongodb.spark.connection.MongoClientCache.logClient(MongoClientCache.scala:161)\n\tat com.mongodb.spark.connection.MongoClientCache.acquire(MongoClientCache.scala:56)\n\tat com.mongodb.spark.MongoConnector.acquireClient(MongoConnector.scala:239)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:152)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator$lzycompute(MongoRDD.scala:217)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator(MongoRDD.scala:217)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:68)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:240)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:229)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:179)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, array, explode, sum, count, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime\n",
    "\n",
    "SYMBOLS_LIST = ['BTC.X', 'BSV.X', 'BCH.X', 'LTC.X', 'ETH.X', 'DOGE.X']\n",
    "\n",
    "# NOTE: The environment needs to have scala installed for this to work\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"myApp\") \\\n",
    ".config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/cryptoracle\") \\\n",
    ".config(\"spark.mongodb.input.collection\", \"messages\") \\\n",
    ".config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.0') \\\n",
    ".getOrCreate()\n",
    "\n",
    "messages_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataframe with only required columns and filtered rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df = messages_df.select(messages_df['_id']['oid'].alias('_id'),\n",
    "                   messages_df['body'],\n",
    "                   messages_df['created_at'],\n",
    "                   messages_df['entities']['sentiment']['basic'].alias('sentiment'),\n",
    "                   messages_df['symbols']['symbol'].alias('symbols'),\n",
    "                   messages_df['likes']['total'].alias('likes'),\n",
    "                   messages_df['reshares']['reshared_count'].alias('reshares'))\n",
    "\n",
    "def sum_interactions(likes, reshares):\n",
    "    _sum = 1\n",
    "    if likes:\n",
    "        _sum += likes\n",
    "    if reshares:\n",
    "        _sum += reshares\n",
    "    return _sum\n",
    "\n",
    "def convert_sentiment(sentiment):\n",
    "    if sentiment == \"Bullish\":\n",
    "        return 2\n",
    "    if sentiment == \"Bearish\":\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_date(dt):\n",
    "    return datetime.strptime(dt, '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d-%H')\n",
    "\n",
    "interactionUdf = udf(sum_interactions, IntegerType())\n",
    "sentimentUdf = udf(convert_sentiment, IntegerType())\n",
    "dateUdf = udf(convert_date, StringType())\n",
    "\n",
    "messages_df = messages_df \\\n",
    "                .withColumn('interaction_count', interactionUdf('likes', 'reshares')) \\\n",
    "                .withColumn('sentiment', sentimentUdf('sentiment')) \\\n",
    "                .withColumn('created_at', dateUdf('created_at'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.groupBy('sentiment').count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unwrap array of symbols to new rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since tweets can be attributed to more than one symbol, we unwrap the list into more rows\n",
    "# this is okay as our final calculation will be grouped by symbol among other things\n",
    "\n",
    "def weight_sentiment(sentiment, count):\n",
    "    if sentiment == 0:\n",
    "        return 0\n",
    "    return sentiment * count\n",
    "\n",
    "def handle_neutral_sentiment(sentiment, count):\n",
    "    if sentiment == 0:\n",
    "        return 0\n",
    "    return count\n",
    "\n",
    "weightedSentimentUdf = udf(weight_sentiment, IntegerType())\n",
    "neutralSentimentUdf = udf(handle_neutral_sentiment, IntegerType())\n",
    "\n",
    "\n",
    "messages_df = messages_df \\\n",
    "                .withColumn('weighted_sentiment', weightedSentimentUdf('sentiment', 'interaction_count')) \\\n",
    "                .withColumn('symbol', explode(messages_df['symbols']))\n",
    "\n",
    "# remove duplicates after exploding\n",
    "messages_df = messages_df.dropDuplicates()\n",
    "\n",
    "# do not consider interaction count for neutral sentiment\n",
    "messages_df = messages_df.withColumn('interaction_count', neutralSentimentUdf('sentiment', 'interaction_count'))\n",
    "# filter to only those symbols that we care about\n",
    "messages_df = messages_df.where(messages_df['symbol'].isin(SYMBOLS_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include counts of sentiment for later aggr in groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_sentiment_count(sentiment):\n",
    "    return (sentiment == 2) ? 1 : 0\n",
    "    \n",
    "def negative_sentiment_count(sentiment):\n",
    "    return (sentiment == 1) ? 1 : 0\n",
    "    \n",
    "def null_sentiment_count(sentiment):\n",
    "    return (sentiment == 0) ? 1 : 0\n",
    "\n",
    "\n",
    "positiveSentimentCountUdf = udf(positive_sentiment_count, IntegerType())\n",
    "negativeSentimentCountUdf = udf(negative_sentiment_count, IntegerType())\n",
    "nullSentimentCountUdf = udf(null_sentiment_count, IntegerType())\n",
    "\n",
    "messages_df = messages_df \\\n",
    "                .withColumn('positive_count', positiveSentimentCountUdf('sentiment')) \\\n",
    "                .withColumn('negative_count', negativeSentimentCountUdf('sentiment')) \\\n",
    "                .withColumn('null_count', nullSentimentCountUdf('sentiment')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.groupby('symbol').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = messages_df.groupby(['created_at', 'symbol']).agg(\n",
    "    sum('interaction_count').alias('sum_interaction_count'),\n",
    "    sum('weighted_sentiment').alias('sum_weighted_sentiment'),\n",
    "    sum('positive_count').alias('sum_positive_count'),\n",
    "    sum('negative_count').alias('sum_negative_count'),\n",
    "    sum('null_count').alias('sum_null_count'),\n",
    "    count('_id').alias('volume_tweets'))\n",
    "\n",
    "grouped_df = grouped_df.withColumn('overall_sentiment', grouped_df['sum_weighted_sentiment'] / grouped_df['sum_interaction_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = grouped_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[gdf['created_at'] == '2020-03-24-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.groupby('symbol').count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "read_func = lambda x: spark.read.format('csv').load(x, header=True, inferSchema=True)\n",
    "\n",
    "def format_date(date, sym):\n",
    "    if sym in ['BCH.X', 'DOGE.X']:\n",
    "        return datetime.strptime(date, '%Y-%m-%d %I-%p').strftime(\"%Y-%m-%d-%H\")\n",
    "    return datetime.strptime(date, '%m/%d/%y %H:%M').strftime(\"%Y-%m-%d-%H\")\n",
    "    \n",
    "\n",
    "formatDateUdf = udf(format_date, StringType())\n",
    "\n",
    "BTC_prices = read_func(\"data/gemini_BTCUSD_1hr.csv\") \n",
    "ETH_prices = read_func(\"data/gemini_ETHUSD_1hr.csv\") \n",
    "LTC_prices = read_func(\"data/gemini_LTCUSD_1hr.csv\") \n",
    "BCH_prices = read_func(\"data/Bitbay_BCHUSD_1h.csv\") \n",
    "DOGE_prices = read_func(\"data/Yobit_DOGERUR_1h.csv\") \n",
    "\n",
    "price_df_lists = [\n",
    "    ('BTC.X', BTC_prices),\n",
    "    ('ETH.X', ETH_prices),\n",
    "    ('LTC.X', LTC_prices),\n",
    "    ('BCH.X', BCH_prices),\n",
    "    ('DOGE.X', DOGE_prices)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Datetime and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfs = []\n",
    "for sym, price_df in price_df_lists:\n",
    "    new_dfs.append(price_df \\\n",
    "                .withColumnRenamed('Symbol', 'symbol') \\\n",
    "                .withColumn('symbol', lit(sym)) \\\n",
    "                .withColumn('Date', formatDateUdf('Date', 'symbol')) \\\n",
    "                .drop('Unix Timestamp'))\n",
    "price_df_lists = new_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Doge (RUR to USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUR_USD = 0.013\n",
    "\n",
    "def convert_doge(price):\n",
    "    return price * RUR_USD\n",
    "\n",
    "convertPriceUdf = udf(convert_doge, DoubleType())\n",
    "\n",
    "for col in ['Open', 'High', 'Low', 'Close']:\n",
    "    price_df_lists[-1] = price_df_lists[-1].withColumn(col, convertPriceUdf(col))\n",
    "    \n",
    "#DOGE\n",
    "price_df_lists[-1] = price_df_lists[-1].drop('Volume ERUR').withColumnRenamed('Volume DOG', 'Volume')\n",
    "#BCH\n",
    "price_df_lists[-2] = price_df_lists[-2].drop('Volume USD').withColumnRenamed('Volume BCH', 'Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in price_df_lists:\n",
    "    print(df.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "unionAll(*price_df_lists).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
