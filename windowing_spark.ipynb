{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Running locally with connector 2.11:2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, array, explode, sum, count, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime\n",
    "\n",
    "SYMBOLS_LIST = ['BTC.X', 'BSV.X', 'BCH.X', 'LTC.X', 'ETH.X', 'DOGE.X']\n",
    "\n",
    "# NOTE: The environment needs to have scala installed for this to work\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"myApp\") \\\n",
    ".config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/cryptoracle\") \\\n",
    ".config(\"spark.mongodb.input.collection\", \"messages\") \\\n",
    ".config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.0') \\\n",
    ".getOrCreate()\n",
    "\n",
    "messages_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Running with remote mongo URI with different connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, array, explode, sum, count, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime\n",
    "\n",
    "MONGODB_INPUT_URI = \"mongodb://heroku_kvptfcm8:vbekldoic9poi92kkp810rvk7@ds141185.mlab.com:41185/heroku_kvptfcm8.runs\"\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.mongodb.input.uri\", MONGODB_INPUT_URI) \\\n",
    "        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:2.4.0') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "messages_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOLS_LIST = ['BTC.X', 'BSV.X', 'BCH.X', 'LTC.X', 'ETH.X', 'DOGE.X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataframe with only required columns and filtered rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df = messages_df.select(messages_df['_id']['oid'].alias('_id'),\n",
    "                   messages_df['body'],\n",
    "                   messages_df['created_at'],\n",
    "                   messages_df['entities']['sentiment']['basic'].alias('sentiment'),\n",
    "                   messages_df['symbols']['symbol'].alias('symbols'),\n",
    "                   messages_df['likes']['total'].alias('likes'),\n",
    "                   messages_df['reshares']['reshared_count'].alias('reshares'))\n",
    "\n",
    "def sum_interactions(likes, reshares):\n",
    "    _sum = 1\n",
    "    if likes:\n",
    "        _sum += likes\n",
    "    if reshares:\n",
    "        _sum += reshares\n",
    "    return _sum\n",
    "\n",
    "def convert_sentiment(sentiment):\n",
    "    if sentiment == \"Bullish\":\n",
    "        return 2\n",
    "    if sentiment == \"Bearish\":\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_date(dt):\n",
    "    return datetime.strptime(dt, '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d-%H')\n",
    "\n",
    "interactionUdf = udf(sum_interactions, IntegerType())\n",
    "sentimentUdf = udf(convert_sentiment, IntegerType())\n",
    "dateHourUdf = udf(convert_date, StringType())\n",
    "\n",
    "# remove null timestamps, id and symbols (else timestamp conversion will have problem)\n",
    "messages_df = messages_df \\\n",
    "                .na \\\n",
    "                .drop(subset=['_id','created_at','symbols'])\n",
    "\n",
    "messages_df = messages_df \\\n",
    "                .withColumn('interaction_count', interactionUdf('likes', 'reshares')) \\\n",
    "                .withColumn('sentiment', sentimentUdf('sentiment')) \\\n",
    "                .withColumn('created_at', dateHourUdf('created_at')) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_id='5e7ad06b80f71592bb059da0', body='$BTC.X When Stalin and the Red Army were closing in on Hitler, He committed suicide, Well the walls are closing in on Trump and I hope for mankind he does the same and takes others with him, I don’t normally wish death as it’s bad karma, But we are dealing with Satan so all bets are off!', created_at='2020-03-25-03', sentiment=0, symbols=['BTC.X'], likes=None, reshares=None, interaction_count=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sentiment=1, count=5567),\n",
       " Row(sentiment=2, count=27348),\n",
       " Row(sentiment=0, count=31231)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df.groupBy('sentiment').count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unwrap array of symbols to new rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since tweets can be attributed to more than one symbol, we unwrap the list into more rows\n",
    "# this is okay as our final calculation will be grouped by symbol among other things\n",
    "\n",
    "def weight_sentiment(sentiment, count):\n",
    "    if sentiment == 0:\n",
    "        return 0\n",
    "    return sentiment * count\n",
    "\n",
    "def handle_neutral_sentiment(sentiment, count):\n",
    "    if sentiment == 0:\n",
    "        return 0\n",
    "    return count\n",
    "\n",
    "weightedSentimentUdf = udf(weight_sentiment, IntegerType())\n",
    "neutralSentimentUdf = udf(handle_neutral_sentiment, IntegerType())\n",
    "\n",
    "\n",
    "messages_df = messages_df \\\n",
    "                .withColumn('weighted_sentiment', weightedSentimentUdf('sentiment', 'interaction_count')) \\\n",
    "                .withColumn('symbol', explode(messages_df['symbols']))\n",
    "\n",
    "# remove duplicates after exploding\n",
    "messages_df = messages_df.dropDuplicates()\n",
    "\n",
    "# do not consider interaction count for neutral sentiment\n",
    "messages_df = messages_df.withColumn('interaction_count', neutralSentimentUdf('sentiment', 'interaction_count'))\n",
    "# filter to only those symbols that we care about\n",
    "messages_df = messages_df.where(messages_df['symbol'].isin(SYMBOLS_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include counts of sentiment for later aggr in groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_sentiment_count(sentiment):\n",
    "    return 1 if (sentiment == 2) else 0\n",
    "    \n",
    "def negative_sentiment_count(sentiment):\n",
    "    return 1 if (sentiment == 1) else 0\n",
    "    \n",
    "def null_sentiment_count(sentiment):\n",
    "    return 1 if (sentiment == 0) else 0\n",
    "\n",
    "\n",
    "positiveSentimentCountUdf = udf(positive_sentiment_count, IntegerType())\n",
    "negativeSentimentCountUdf = udf(negative_sentiment_count, IntegerType())\n",
    "nullSentimentCountUdf = udf(null_sentiment_count, IntegerType())\n",
    "\n",
    "messages_df = messages_df \\\n",
    "                .withColumn('positive', positiveSentimentCountUdf('sentiment')) \\\n",
    "                .withColumn('negative', negativeSentimentCountUdf('sentiment')) \\\n",
    "                .withColumn('null', nullSentimentCountUdf('sentiment')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(symbol='BCH.X', count=12585),\n",
       " Row(symbol='BTC.X', count=27300),\n",
       " Row(symbol='ETH.X', count=17796),\n",
       " Row(symbol='DOGE.X', count=11044),\n",
       " Row(symbol='BSV.X', count=11805),\n",
       " Row(symbol='LTC.X', count=14367)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df.groupby('symbol').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = messages_df.groupby(['created_at', 'symbol']).agg(\n",
    "    sum('interaction_count').alias('sum_interaction_count'),\n",
    "    sum('weighted_sentiment').alias('sum_weighted_sentiment'),\n",
    "    sum('positive').alias('positive_count'),\n",
    "    sum('negative').alias('negative_count'),\n",
    "    sum('null').alias('null_count'),\n",
    "    count('_id').alias('volume_tweets'))\n",
    "\n",
    "grouped_df = grouped_df.withColumn('overall_sentiment', grouped_df['sum_weighted_sentiment'] / grouped_df['sum_interaction_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = grouped_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>symbol</th>\n",
       "      <th>sum_interaction_count</th>\n",
       "      <th>sum_weighted_sentiment</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>null_count</th>\n",
       "      <th>volume_tweets</th>\n",
       "      <th>overall_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>2020-03-24-23</td>\n",
       "      <td>BSV.X</td>\n",
       "      <td>70</td>\n",
       "      <td>140</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2020-03-24-23</td>\n",
       "      <td>BCH.X</td>\n",
       "      <td>239</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.004184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>2020-03-24-23</td>\n",
       "      <td>ETH.X</td>\n",
       "      <td>249</td>\n",
       "      <td>260</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>1.044177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>2020-03-24-23</td>\n",
       "      <td>LTC.X</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>2020-03-24-23</td>\n",
       "      <td>BTC.X</td>\n",
       "      <td>249</td>\n",
       "      <td>260</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>1.044177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_at symbol  sum_interaction_count  sum_weighted_sentiment  \\\n",
       "642   2020-03-24-23  BSV.X                     70                     140   \n",
       "1435  2020-03-24-23  BCH.X                    239                     240   \n",
       "2581  2020-03-24-23  ETH.X                    249                     260   \n",
       "2680  2020-03-24-23  LTC.X                    238                     238   \n",
       "3043  2020-03-24-23  BTC.X                    249                     260   \n",
       "\n",
       "      positive_count  negative_count  null_count  volume_tweets  \\\n",
       "642               14               0           0             14   \n",
       "1435               1              34           0             35   \n",
       "2581               4              34           4             42   \n",
       "2680               0              34          10             44   \n",
       "3043               4              34          11             49   \n",
       "\n",
       "      overall_sentiment  \n",
       "642            2.000000  \n",
       "1435           1.004184  \n",
       "2581           1.044177  \n",
       "2680           1.000000  \n",
       "3043           1.044177  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf[gdf['created_at'] == '2020-03-24-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(symbol='BCH.X', count=311),\n",
       " Row(symbol='BTC.X', count=903),\n",
       " Row(symbol='ETH.X', count=786),\n",
       " Row(symbol='DOGE.X', count=337),\n",
       " Row(symbol='BSV.X', count=367),\n",
       " Row(symbol='LTC.X', count=552)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.groupby('symbol').count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "read_func = lambda x: spark.read.format('csv').load(x, header=True, inferSchema=True)\n",
    "\n",
    "def format_date(date, sym):\n",
    "    if sym in ['BCH.X', 'DOGE.X']:\n",
    "        return datetime.strptime(date, '%Y-%m-%d %I-%p').strftime(\"%Y-%m-%d-%H\")\n",
    "    return datetime.strptime(date, '%m/%d/%y %H:%M').strftime(\"%Y-%m-%d-%H\")\n",
    "\n",
    "# needed for join with crypto data\n",
    "def only_date(date):\n",
    "    return datetime.strptime(date, \"%Y-%m-%d-%H\").strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "formatDateUdf = udf(format_date, StringType())\n",
    "dateUdf = udf(only_date, StringType())\n",
    "\n",
    "BTC_prices = read_func(\"data/gemini_BTCUSD_1hr.csv\") \n",
    "ETH_prices = read_func(\"data/gemini_ETHUSD_1hr.csv\") \n",
    "LTC_prices = read_func(\"data/gemini_LTCUSD_1hr.csv\") \n",
    "BCH_prices = read_func(\"data/Bitbay_BCHUSD_1h.csv\") \n",
    "DOGE_prices = read_func(\"data/Yobit_DOGEUSD_1h.csv\") \n",
    "\n",
    "price_df_lists = [\n",
    "    ('BTC.X', BTC_prices),\n",
    "    ('ETH.X', ETH_prices),\n",
    "    ('LTC.X', LTC_prices),\n",
    "    ('BCH.X', BCH_prices),\n",
    "    ('DOGE.X', DOGE_prices)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Datetime and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfs = []\n",
    "for sym, price_df in price_df_lists:\n",
    "    new_dfs.append(price_df \\\n",
    "                .withColumn('Symbol', lit(sym)) \\\n",
    "                .withColumn('Date', formatDateUdf('Date', 'Symbol')) \\\n",
    "                .withColumn('Only_date', dateUdf('Date'))\n",
    "                .drop('Unix Timestamp'))\n",
    "price_df_lists = new_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. No longer needed: Doge (RUR to USD)\n",
    "<!-- RUR_USD = 0.013\n",
    "\n",
    "def convert_doge(price):\n",
    "    return price * RUR_USD\n",
    "\n",
    "convertPriceUdf = udf(convert_doge, DoubleType())\n",
    "\n",
    "for col in ['Open', 'High', 'Low', 'Close']:\n",
    "    price_df_lists[-1] = price_df_lists[-1].withColumn(col, convertPriceUdf(col))\n",
    "    \n",
    "#DOGE\n",
    "price_df_lists[-1] = price_df_lists[-1].drop('Volume ERUR').withColumnRenamed('Volume DOG', 'Volume')\n",
    "#BCH\n",
    "price_df_lists[-2] = price_df_lists[-2].drop('Volume USD').withColumnRenamed('Volume BCH', 'Volume') -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Rename Volume columns for BCH and DOGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOGE\n",
    "price_df_lists[-1] = price_df_lists[-1].drop('Volume EUSD').withColumnRenamed('Volume DOG', 'Volume')\n",
    "#BCH\n",
    "price_df_lists[-2] = price_df_lists[-2].drop('Volume USD').withColumnRenamed('Volume BCH', 'Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get VWAP price out of OHLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vwap(Open, High, Low, Close):\n",
    "    return (Open + High + (Low + Close)/2)/3\n",
    "    \n",
    "convertVwapUdf = udf(convert_vwap, DoubleType())\n",
    "\n",
    "for index in range(len(price_df_lists)):\n",
    "    df = price_df_lists[index]\n",
    "    price_df_lists[index] = df.withColumn('VWAP', convertVwapUdf('Open', 'High', 'Low', 'Close')) \\\n",
    "                              .drop('Open') \\\n",
    "                              .drop('Low') \\\n",
    "                              .drop('High') \\\n",
    "                              .drop('Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date='2020-05-02-00', Symbol='BTC.X', Volume=25.78443763, Only_date='2020-05-02', VWAP=8806.521666666667)\n",
      "Row(Date='2020-05-02-00', Symbol='ETH.X', Volume=41.342262, Only_date='2020-05-02', VWAP=212.08166666666668)\n",
      "Row(Date='2020-05-02-00', Symbol='LTC.X', Volume=176.431, Only_date='2020-05-02', VWAP=47.080000000000005)\n",
      "Row(Date='2020-04-29-12', Symbol='BCH.X', Volume=0.08969, Only_date='2020-04-29', VWAP=252.42)\n",
      "Row(Date='2018-08-22-23', Symbol='DOGE.X', Volume=0.0, Only_date='2018-08-22', VWAP=0.002432)\n"
     ]
    }
   ],
   "source": [
    "for df in price_df_lists:\n",
    "    print(df.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "prices_df = unionAll(*price_df_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining tweets with price data (tweets are k-hour earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [2, 5, 12, 24, 48]\n",
    "tweetsprice_df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched tweet group for k = 2: 0\n",
      "Number of matched tweet groups for k = 2: 2484\n",
      "\n",
      "Number of unmatched tweet group for k = 5: 0\n",
      "Number of matched tweet groups for k = 5: 2484\n",
      "\n",
      "Number of unmatched tweet group for k = 12: 0\n",
      "Number of matched tweet groups for k = 12: 2484\n",
      "\n",
      "Number of unmatched tweet group for k = 24: 0\n",
      "Number of matched tweet groups for k = 24: 2484\n",
      "\n",
      "Number of unmatched tweet group for k = 48: 0\n",
      "Number of matched tweet groups for k = 48: 2484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "for k in k_list:\n",
    "    merged_df = grouped_df.join(prices_df, \\\n",
    "                               (grouped_df.symbol == prices_df.Symbol)&(grouped_df.created_at == prices_df.Date), \\\n",
    "                                how='inner').drop(prices_df.Symbol) \n",
    "    unmatched_count = merged_df.filter((merged_df[\"Date\"] == \"\") | \n",
    "                                       merged_df[\"Date\"].isNull() | \n",
    "                                       isnan(merged_df[\"Date\"])).count()\n",
    "    print(\"Number of unmatched tweet group for k = \" + str(k) + \": \"+ str(unmatched_count))\n",
    "    print(\"Number of matched tweet groups for k = \" + str(k) + \": \"+ str(merged_df.count() - unmatched_count) + \"\\n\")\n",
    "    tweetsprice_df_list.append(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cryptocurrency-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_data = read_func(\"data/btc.csv\") \n",
    "ETH_data = read_func(\"data/eth.csv\") \n",
    "LTC_data = read_func(\"data/ltc.csv\") \n",
    "BCH_data = read_func(\"data/bch.csv\") \n",
    "DOGE_data = read_func(\"data/doge.csv\") \n",
    "\n",
    "cryptospec_df_lists = [\n",
    "    ('BTC.X', BTC_data),\n",
    "    ('ETH.X', ETH_data),\n",
    "    ('LTC.X', LTC_data),\n",
    "    ('BCH.X', BCH_data),\n",
    "    ('DOGE.X', DOGE_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that we are interested in and commonly refered in cryptocurrency data \n",
    "# Transaction count, volume, fees\n",
    "# Network value, Realized cap \n",
    "# Active addresses: ADA.AdrActCnt\n",
    "# Payment count, Average difficulty: DiffMean, Hash rate\n",
    "# Block size, Block count, Current supply\n",
    "\n",
    "common_col = ['date', 'AdrActCnt', 'TxCnt', 'TxTfrValAdjUSD', 'ROI30d', 'HashRate', \n",
    "              'BlkCnt', 'VtyDayRet180d', 'CapMrktCurUSD', 'SplyCur', 'ROI1yr',\n",
    "              'BlkSizeMeanByte','VtyDayRet60d','VtyDayRet30d', 'FeeTotUSD',\n",
    "              'DiffMean']\n",
    "\n",
    "# make sure all the crypto_df has the columns before union\n",
    "for sym, crypto_df in cryptospec_df_lists:\n",
    "    common_col = list(set(common_col) & set(crypto_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_dfs = []\n",
    "for sym, crypto_df in cryptospec_df_lists:\n",
    "    crypto_dfs.append(crypto_df \\\n",
    "                .select([col for col in crypto_df.columns if col in common_col])  \n",
    "                .where(crypto_df['date'] > lit(\"2020-03-10\")) \n",
    "                .withColumnRenamed('coin', 'symbol') \\\n",
    "                .withColumn('symbol', lit(sym)))\n",
    "    \n",
    "cryptos_df = unionAll(*crypto_dfs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining tweets-price with crypto-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetspricecrypto_df_list = []\n",
    "\n",
    "for tweetsprice in tweetsprice_df_list:\n",
    "    merged_df = tweetsprice.join(cryptos_df, \\\n",
    "                               (tweetsprice.symbol == cryptos_df.symbol)&(tweetsprice.Only_date == cryptos_df.date), \\\n",
    "                                how='inner') \\\n",
    "                           .drop(prices_df.Only_date) \\\n",
    "                           .drop(cryptos_df.date) \\\n",
    "                           .drop(cryptos_df.symbol)\n",
    "    tweetspricecrypto_df_list.append(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(k_list)):\n",
    "    filename = 'output/' + str(k_list[index]) + \"h.csv\"\n",
    "    tweetspricecrypto_df_list[index].coalesce(1).write.option(\"header\", \"true\").csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for machine learning\n",
    "(source: https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'house_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d7df96a15502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtransformed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweetspricecrypto_df\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweetspricecrypto_df_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorAssembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhouse_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvhouse_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VWAP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtransformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'house_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "inputCols = list(tweetspricecrypto_df_list[0].columns).remove('VWAP')\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = inputCols, outputCol = 'features')\n",
    "\n",
    "transformed_list = []\n",
    "for tweetspricecrypto_df in tweetspricecrypto_df_list:\n",
    "    transformed = vectorAssembler.transform(house_df)\n",
    "    transformed = vhouse_df.select(['features', 'VWAP'])\n",
    "    transformed.show(3)\n",
    "    transfomed_list.append(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr_model_list = []\n",
    "for transformed_df in transformed_list:\n",
    "    lr = LinearRegression(featuresCol = 'features', labelCol='MV', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    lr_model = lr.fit(transformed_df)\n",
    "    lr_model_list.append(lr_model)\n",
    "    print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "    print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_model_list)):\n",
    "    lr_model = lr_model_list[i]\n",
    "    trainingSummary = lr_model.summary\n",
    "    print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "    print(\"r2: %f\" % trainingSummary.r2)\n",
    "    transformed_list[i].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"MV\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"MV\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
