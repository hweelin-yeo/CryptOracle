{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, array, explode, sum, count, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:2.4.0') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_func = lambda x: spark.read.format('csv').load(x, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [2, 5, 12, 24, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoH_data = read_func(\"intermediate/2h.csv\") \n",
    "fiveH_data = read_func(\"intermediate/5h.csv\") \n",
    "twelveH_data = read_func(\"intermediate/12h.csv\") \n",
    "twoFourH_data = read_func(\"intermediate/24h.csv\") \n",
    "foureightH_data = read_func(\"intermediate/48h.csv\") \n",
    "\n",
    "tweetsprice_df_list = [\n",
    "    (2, twoH_data),\n",
    "    (5, fiveH_data),\n",
    "    (12, twelveH_data),\n",
    "    (24, twoFourH_data),\n",
    "    (48, foureightH_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency spec data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_data = read_func(\"data/btc.csv\") \n",
    "ETH_data = read_func(\"data/eth.csv\") \n",
    "LTC_data = read_func(\"data/ltc.csv\") \n",
    "BCH_data = read_func(\"data/bch.csv\") \n",
    "DOGE_data = read_func(\"data/doge.csv\") \n",
    "\n",
    "cryptospec_df_lists = [\n",
    "    ('BTC.X', BTC_data),\n",
    "    ('ETH.X', ETH_data),\n",
    "    ('LTC.X', LTC_data),\n",
    "    ('BCH.X', BCH_data),\n",
    "    ('DOGE.X', DOGE_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that we are interested in and commonly refered in cryptocurrency data \n",
    "# Transaction count, volume, fees\n",
    "# Network value, Realized cap \n",
    "# Active addresses: ADA.AdrActCnt\n",
    "# Payment count, Average difficulty: DiffMean, Hash rate\n",
    "# Block size, Block count, Current supply\n",
    "\n",
    "common_col = ['date', 'AdrActCnt', 'TxCnt', 'TxTfrValAdjUSD', 'ROI30d', 'HashRate', \n",
    "              'BlkCnt', 'VtyDayRet180d', 'CapMrktCurUSD', 'SplyCur', 'ROI1yr',\n",
    "              'BlkSizeMeanByte','VtyDayRet60d','VtyDayRet30d', 'FeeTotUSD',\n",
    "              'DiffMean']\n",
    "\n",
    "# make sure all the crypto_df has the columns before union\n",
    "for sym, crypto_df in cryptospec_df_lists:\n",
    "    common_col = list(set(common_col) & set(crypto_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_dfs = []\n",
    "for sym, crypto_df in cryptospec_df_lists:\n",
    "    crypto_dfs.append(crypto_df \\\n",
    "                .select([col for col in crypto_df.columns if col in common_col])  \n",
    "                .where(crypto_df['date'] > lit(\"2020-03-10\")) \n",
    "                .withColumnRenamed('coin', 'symbol') \\\n",
    "                .withColumn('symbol', lit(sym)))\n",
    "    \n",
    "cryptos_df = unionAll(*crypto_dfs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_symbol(symbol):\n",
    "    if (symbol == 'BTC.X'):\n",
    "        return 0\n",
    "    elif (symbol == 'BCH.X'):\n",
    "        return 1\n",
    "    elif (symbol == 'LTC.X'):\n",
    "        return 2\n",
    "    elif (symbol == 'ETH.X'):\n",
    "        return 3\n",
    "    elif (symbol == 'DOGE.X'):\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "symbolUdf = udf(convert_symbol, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetspricecrypto_df_list = []\n",
    "\n",
    "for hour, tweetsprice in tweetsprice_df_list:\n",
    "    merged_df = tweetsprice.join(cryptos_df, \\\n",
    "                               (tweetsprice.Symbol == cryptos_df.symbol)&(tweetsprice.Only_date == cryptos_df.date), \\\n",
    "                                how='inner') \\\n",
    "                           .drop(tweetsprice.Only_date) \\\n",
    "                           .drop(cryptos_df.date) \\\n",
    "                           .drop(cryptos_df.symbol) \\\n",
    "                           .withColumn('Symbol', symbolUdf('Symbol'))\n",
    "    tweetspricecrypto_df_list.append(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(k_list)):\n",
    "    filename = 'output/' + str(k_list[index]) + \"h.csv\"\n",
    "    tweetspricecrypto_df_list[index].coalesce(1).write.option(\"header\", \"true\").csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
